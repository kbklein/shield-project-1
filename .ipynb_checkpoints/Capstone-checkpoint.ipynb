{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Capstone.ipynb - Local version (no Google Colab references)\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (accuracy_score, recall_score, precision_score,\n",
    "                             confusion_matrix, classification_report)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pickle\n",
    "\n",
    "# Ensure we see all columns in DataFrame outputs\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# ==============================\n",
    "# 1. LOAD THE DATASET LOCALLY\n",
    "# ==============================\n",
    "# Replace \"dataset_phishing.csv\" with the exact name of your local file\n",
    "df = pd.read_csv(\"dataset_phishing.csv\")  \n",
    "print(\"First 5 rows of the dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# ============================\n",
    "# 2. BASIC CLEANING / CHECKS\n",
    "# ============================\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isna().sum())\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "print(f\"\\nData shape after dropping missing values: {df.shape}\")\n",
    "\n",
    "# ================================\n",
    "# 3. FEATURE AND TARGET PREP\n",
    "# ================================\n",
    "features = [\n",
    "    'length_url', 'length_hostname', 'ip', 'nb_dots', 'nb_hyphens', 'nb_at',\n",
    "    'nb_qm', 'nb_and', 'nb_or', 'nb_eq', 'nb_underscore', 'nb_tilde',\n",
    "    'nb_percent', 'nb_slash', 'nb_star', 'nb_colon', 'nb_comma',\n",
    "    'nb_semicolumn', 'nb_dollar', 'nb_space', 'nb_www', 'nb_com',\n",
    "    'nb_dslash', 'http_in_path', 'https_token', 'ratio_digits_url',\n",
    "    'ratio_digits_host', 'punycode', 'shortening_service',\n",
    "    'path_extension', 'phish_hints', 'domain_in_brand',\n",
    "    'brand_in_subdomain', 'brand_in_path', 'suspecious_tld'\n",
    "]\n",
    "\n",
    "# Convert target from string to numeric (phishing=1, legitimate=0)\n",
    "df['status'] = df['status'].map({'phishing': 1, 'legitimate': 0})\n",
    "\n",
    "print(\"\\nValue counts of status (0=legitimate, 1=phishing):\")\n",
    "print(df['status'].value_counts())\n",
    "\n",
    "# =========================\n",
    "# 4. CORRELATION ANALYSIS\n",
    "# =========================\n",
    "# Only keep numerical columns for correlation\n",
    "numerical_df = df.select_dtypes(include=['float64', 'int64'])\n",
    "corr_matrix = numerical_df.corr()\n",
    "status_corr = corr_matrix['status']\n",
    "\n",
    "# Quick function to filter features above a certain correlation threshold\n",
    "def feature_selector_correlation(cmatrix, threshold):\n",
    "    selected_features = []\n",
    "    feature_score = []\n",
    "    for i, score in enumerate(cmatrix):\n",
    "        if abs(score) > threshold:\n",
    "            selected_features.append(cmatrix.index[i])\n",
    "            feature_score.append(['{:3f}'.format(score)])\n",
    "    return list(zip(selected_features, feature_score))\n",
    "\n",
    "features_selected = feature_selector_correlation(status_corr, 0.2)\n",
    "print(\"\\nFeatures with correlation above 0.2:\")\n",
    "print(features_selected)\n",
    "\n",
    "selected_features = [\n",
    "    f for f, _ in features_selected \n",
    "    if f != 'status'\n",
    "]\n",
    "\n",
    "# ================================\n",
    "# 5. TRAIN-TEST SPLIT & SCALING\n",
    "# ================================\n",
    "X = df[selected_features]\n",
    "y = df['status']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# =======================\n",
    "# 6. MODEL TRAINING\n",
    "# =======================\n",
    "classifiers = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(),\n",
    "    'SVM': SVC(),\n",
    "    'KNN': KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "param_grids = {\n",
    "    'Logistic Regression': {\n",
    "        'C': [0.1, 1, 10]\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [None, 10, 20]\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 1]\n",
    "    },\n",
    "    'SVM': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'kernel': ['linear', 'rbf']\n",
    "    },\n",
    "    'KNN': {\n",
    "        'n_neighbors': [3, 5, 7, 9],\n",
    "        'p': [1, 2]\n",
    "    }\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, clf in classifiers.items():\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=clf,\n",
    "        param_grid=param_grids[name],\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "        scoring='accuracy'\n",
    "    )\n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "    results[name] = grid_search\n",
    "\n",
    "# Show best hyperparams & test performance\n",
    "for name, grid_search in results.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(\"Best Parameters:\", grid_search.best_params_)\n",
    "    print(\"Best Score (CV):\", grid_search.best_score_)\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test_scaled)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Test Accuracy:\", test_accuracy)\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\n=== Summary of Best Models ===\")\n",
    "for name, grid_search in results.items():\n",
    "    print(f\"{name} -> Best Params: {grid_search.best_params_}, CV Score: {grid_search.best_score_}\")\n",
    "\n",
    "# ===============================\n",
    "# 7. FINAL MODEL (EXAMPLE)\n",
    "# ===============================\n",
    "print(\"\\n--- Training a final RandomForest with chosen hyperparams ---\")\n",
    "model = RandomForestClassifier(max_depth=20, n_estimators=100)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ===============================\n",
    "# 8. SAVE MODEL & SCALER LOCALLY\n",
    "# ===============================\n",
    "with open('phishing_model.pkl', 'wb') as model_file:\n",
    "    pickle.dump(model, model_file)\n",
    "\n",
    "with open('scaler.pkl', 'wb') as scaler_file:\n",
    "    pickle.dump(scaler, scaler_file)\n",
    "\n",
    "print(\"\\nModel and scaler saved to 'phishing_model.pkl' and 'scaler.pkl'.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
